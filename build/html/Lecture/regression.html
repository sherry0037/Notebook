

<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Regression Analysis &#8212; Notebook  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/yeen.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="JavaServer Pages (JSP) Tutorial" href="../Java/jsp.html" />
    <link rel="prev" title="Neural Networks" href="neural_networks.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../Java/jsp.html" title="JavaServer Pages (JSP) Tutorial"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="neural_networks.html" title="Neural Networks"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Notebook  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Regression Analysis</a></li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Regression Analysis</a><ul>
<li><a class="reference internal" href="#chapter-1-linear-regression-with-one-predictor-variable">Chapter 1: Linear Regression with One Predictor Variable</a><ul>
<li><a class="reference internal" href="#simple-linear-regression-model">Simple Linear Regression Model</a><ul>
<li><a class="reference internal" href="#model-definition">Model definition</a></li>
<li><a class="reference internal" href="#meaning-of-regression-parameters">Meaning of regression parameters</a></li>
</ul>
</li>
<li><a class="reference internal" href="#estimation-of-regression-function">Estimation of Regression Function</a><ul>
<li><a class="reference internal" href="#method-of-least-squares">Method of least squares</a></li>
<li><a class="reference internal" href="#point-estimation-of-mean-response">Point estimation of mean response</a></li>
<li><a class="reference internal" href="#residuals">Residuals</a></li>
</ul>
</li>
<li><a class="reference internal" href="#estimation-of-error-terms-variance-sigma-2">Estimation of Error Terms Variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></a><ul>
<li><a class="reference internal" href="#point-estimator-of-sigma-2">Point estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#normally-distributed-error">Normally Distributed Error</a></li>
</ul>
</li>
<li><a class="reference internal" href="#chapter-2-inferences-in-regression-and-correlation-analysis">Chapter 2: Inferences in Regression and Correlation Analysis</a><ul>
<li><a class="reference internal" href="#inferences-on-beta-1">Inferences on <span class="math notranslate nohighlight">\(\beta_1\)</span></a><ul>
<li><a class="reference internal" href="#sampling-distribution-of-beta-1">Sampling distribution of <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li><a class="reference internal" href="#distribution-of-b-1-beta-1-s-b-1">Distribution of <span class="math notranslate nohighlight">\((b_1 - \beta_1) / s[b_1]\)</span></a></li>
<li><a class="reference internal" href="#confidence-interval-for-beta-1">Confidence interval for <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li><a class="reference internal" href="#hypothesis-test-for-beta-1">Hypothesis test for <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#inferences-on-beta-0">Inferences on <span class="math notranslate nohighlight">\(\beta_0\)</span></a><ul>
<li><a class="reference internal" href="#id1">Sampling distribution of <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li><a class="reference internal" href="#confidence-interval-for-beta-0">Confidence interval for <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#power-of-tests">Power of Tests</a></li>
<li><a class="reference internal" href="#interval-estimation-of-e-y-h">Interval Estimation of <span class="math notranslate nohighlight">\(E[Y_h]\)</span></a><ul>
<li><a class="reference internal" href="#sampling-distribution-of-hat-y-h">Sampling Distribution of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span></a></li>
<li><a class="reference internal" href="#confidence-interval-for-e-y-h">Confidence interval for <span class="math notranslate nohighlight">\(E[Y_h]\)</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#prediction-of-new-observation">Prediction of New Observation</a></li>
<li><a class="reference internal" href="#confidence-band-for-regression-line">Confidence Band for Regression Line</a></li>
<li><a class="reference internal" href="#anova-analysis-of-variance-approach-to-regression-analysis">ANOVA (Analysis of Variance) Approach to Regression Analysis</a><ul>
<li><a class="reference internal" href="#partitioning-of-total-sum-of-squares">Partitioning of Total Sum of Squares</a></li>
<li><a class="reference internal" href="#breakdown-of-degrees-of-freedom">Breakdown of Degrees of Freedom</a></li>
<li><a class="reference internal" href="#mean-squares">Mean Squares</a></li>
<li><a class="reference internal" href="#anova-table">ANOVA table</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="neural_networks.html"
                        title="previous chapter">Neural Networks</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../Java/jsp.html"
                        title="next chapter">JavaServer Pages (JSP) Tutorial</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/Lecture/regression.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="regression-analysis">
<h1><a class="toc-backref" href="#id2">Regression Analysis</a><a class="headerlink" href="#regression-analysis" title="Permalink to this headline">Â¶</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#regression-analysis" id="id2">Regression Analysis</a></p>
<ul>
<li><p><a class="reference internal" href="#chapter-1-linear-regression-with-one-predictor-variable" id="id3">Chapter 1: Linear Regression with One Predictor Variable</a></p>
<ul>
<li><p><a class="reference internal" href="#simple-linear-regression-model" id="id4">Simple Linear Regression Model</a></p>
<ul>
<li><p><a class="reference internal" href="#model-definition" id="id5">Model definition</a></p></li>
<li><p><a class="reference internal" href="#meaning-of-regression-parameters" id="id6">Meaning of regression parameters</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#estimation-of-regression-function" id="id7">Estimation of Regression Function</a></p>
<ul>
<li><p><a class="reference internal" href="#method-of-least-squares" id="id8">Method of least squares</a></p></li>
<li><p><a class="reference internal" href="#point-estimation-of-mean-response" id="id9">Point estimation of mean response</a></p></li>
<li><p><a class="reference internal" href="#residuals" id="id10">Residuals</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#estimation-of-error-terms-variance-sigma-2" id="id11">Estimation of Error Terms Variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#point-estimator-of-sigma-2" id="id12">Point estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#normally-distributed-error" id="id13">Normally Distributed Error</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#chapter-2-inferences-in-regression-and-correlation-analysis" id="id14">Chapter 2: Inferences in Regression and Correlation Analysis</a></p>
<ul>
<li><p><a class="reference internal" href="#inferences-on-beta-1" id="id15">Inferences on <span class="math notranslate nohighlight">\(\beta_1\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#sampling-distribution-of-beta-1" id="id16">Sampling distribution of <span class="math notranslate nohighlight">\(\beta_1\)</span></a></p></li>
<li><p><a class="reference internal" href="#distribution-of-b-1-beta-1-s-b-1" id="id17">Distribution of <span class="math notranslate nohighlight">\((b_1 - \beta_1) / s[b_1]\)</span></a></p></li>
<li><p><a class="reference internal" href="#confidence-interval-for-beta-1" id="id18">Confidence interval for <span class="math notranslate nohighlight">\(\beta_1\)</span></a></p></li>
<li><p><a class="reference internal" href="#hypothesis-test-for-beta-1" id="id19">Hypothesis test for <span class="math notranslate nohighlight">\(\beta_1\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#inferences-on-beta-0" id="id20">Inferences on <span class="math notranslate nohighlight">\(\beta_0\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id21">Sampling distribution of <span class="math notranslate nohighlight">\(\beta_1\)</span></a></p></li>
<li><p><a class="reference internal" href="#confidence-interval-for-beta-0" id="id22">Confidence interval for <span class="math notranslate nohighlight">\(\beta_0\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#power-of-tests" id="id23">Power of Tests</a></p></li>
<li><p><a class="reference internal" href="#interval-estimation-of-e-y-h" id="id24">Interval Estimation of <span class="math notranslate nohighlight">\(E[Y_h]\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#sampling-distribution-of-hat-y-h" id="id25">Sampling Distribution of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span></a></p></li>
<li><p><a class="reference internal" href="#confidence-interval-for-e-y-h" id="id26">Confidence interval for <span class="math notranslate nohighlight">\(E[Y_h]\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#prediction-of-new-observation" id="id27">Prediction of New Observation</a></p></li>
<li><p><a class="reference internal" href="#confidence-band-for-regression-line" id="id28">Confidence Band for Regression Line</a></p></li>
<li><p><a class="reference internal" href="#anova-analysis-of-variance-approach-to-regression-analysis" id="id29">ANOVA (Analysis of Variance) Approach to Regression Analysis</a></p>
<ul>
<li><p><a class="reference internal" href="#partitioning-of-total-sum-of-squares" id="id30">Partitioning of Total Sum of Squares</a></p></li>
<li><p><a class="reference internal" href="#breakdown-of-degrees-of-freedom" id="id31">Breakdown of Degrees of Freedom</a></p></li>
<li><p><a class="reference internal" href="#mean-squares" id="id32">Mean Squares</a></p></li>
<li><p><a class="reference internal" href="#anova-table" id="id33">ANOVA table</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<section id="chapter-1-linear-regression-with-one-predictor-variable">
<h2><a class="toc-backref" href="#id3">Chapter 1: Linear Regression with One Predictor Variable</a><a class="headerlink" href="#chapter-1-linear-regression-with-one-predictor-variable" title="Permalink to this headline">Â¶</a></h2>
<section id="simple-linear-regression-model">
<h3><a class="toc-backref" href="#id4">Simple Linear Regression Model</a><a class="headerlink" href="#simple-linear-regression-model" title="Permalink to this headline">Â¶</a></h3>
<section id="model-definition">
<h4><a class="toc-backref" href="#id5">Model definition</a><a class="headerlink" href="#model-definition" title="Permalink to this headline">Â¶</a></h4>
<p>This is a basic regression model with one predictor variable and the function is linear:</p>
<div class="math notranslate nohighlight">
\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\]</div>
<dl class="simple">
<dt>where:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y_i\)</span> is the value of the response variable in the ith trial</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(X_i\)</span> is the value of the predictor variable in the ith trial. <span class="math notranslate nohighlight">\(X_i\)</span> is a constant</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_i\)</span> is a random error term with mean <span class="math notranslate nohighlight">\(E[\epsilon_i] = 0\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2[\epsilon_i] = \sigma^2\)</span></p></li>
</ul>
</dd>
</dl>
<p>For this model, there are some importance features:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_1 X_i\)</span> is a constant term, <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is a random term, hence <span class="math notranslate nohighlight">\(Y_i\)</span> is a random variable</p></li>
<li><p><span class="math notranslate nohighlight">\(E[Y_i] = \beta_0 + \beta_1 X_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^2[Y_i] = \sigma^2[\epsilon_i] = \sigma^2\)</span></p></li>
<li><p>The error terms are uncorrelated to each other, so are the response <span class="math notranslate nohighlight">\(Y_i\)</span></p></li>
</ol>
<p>The <strong>regression function</strong>, by definition, relates the means of the probability distributions of Y for given X to the level of X. Hence, the regression function for this model is:</p>
<div class="math notranslate nohighlight">
\[E[Y] = \beta_0 + \beta_1 X\]</div>
<div class="topic">
<p class="topic-title">A Simple Linear Regression Model</p>
<p>A consultant is studying the relationship between the number of bids requested by construction contractors for basic lighting equipment during a week and the time required to prepare the bids. Suppose the regression model is:</p>
<div class="math notranslate nohighlight">
\[Y_i = 9.5 + 2.1X_i + \epsilon_i\]</div>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/figure1_6.png"><img alt="../_images/figure1_6.png" src="../_images/figure1_6.png" style="width: 534.4px; height: 228.0px;" /></a>
</figure>
</div>
</section>
<section id="meaning-of-regression-parameters">
<h4><a class="toc-backref" href="#id6">Meaning of regression parameters</a><a class="headerlink" href="#meaning-of-regression-parameters" title="Permalink to this headline">Â¶</a></h4>
<p>The parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are <em>regression coefficients</em>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the <em>slope</em> of the regression line. It indicates the change in the mean of <span class="math notranslate nohighlight">\(Y\)</span> per unit increase in <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><dl class="simple">
<dt><span class="math notranslate nohighlight">\(\beta_1\)</span> is the <span class="math notranslate nohighlight">\(Y\)</span> <em>intercept</em> of the regression line. When the scope of the model includes <span class="math notranslate nohighlight">\(X=0\)</span>, <span class="math notranslate nohighlight">\(\beta_0\)</span> gives the mean of the probability distribution of <span class="math notranslate nohighlight">\(Y\)</span> at <span class="math notranslate nohighlight">\(X=0\)</span>.</dt><dd><ul>
<li><p>Note: when the scope of the model does not cover <span class="math notranslate nohighlight">\(X = 0\)</span>, the intercept does not have any particular meaning. For example, we cannot say the time to build a lot with 0 size is a positive number.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
</section>
<section id="estimation-of-regression-function">
<h3><a class="toc-backref" href="#id7">Estimation of Regression Function</a><a class="headerlink" href="#estimation-of-regression-function" title="Permalink to this headline">Â¶</a></h3>
<section id="method-of-least-squares">
<h4><a class="toc-backref" href="#id8">Method of least squares</a><a class="headerlink" href="#method-of-least-squares" title="Permalink to this headline">Â¶</a></h4>
<p>We use the method of least squares to find estimators of the regression parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. This method considers the sum of the squared deviations of each observations <span class="math notranslate nohighlight">\(Y_i\)</span> from its expected value <span class="math notranslate nohighlight">\(E[Y_i]\)</span>:</p>
<div class="math notranslate nohighlight">
\[Q = \sum_n^{i=1} (Y_i - \beta_0 -\beta_1X_i)^2\]</div>
<p>The estimators of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are the values <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> that <strong>minimize</strong> the criterion <span class="math notranslate nohighlight">\(Q\)</span> for a given sample observations <span class="math notranslate nohighlight">\((X_1, Y_1), ... (X_n, Y_n)\)</span>. <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> are called <em>point estimators</em> of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, respectively.</p>
<div class="topic">
<p class="topic-title">Example</p>
<p>In a small-scale study of persistence, an experimenter gave three subjects a very difficult task. Data on the age of the subject (X) and on the number of attempts to accomplish the task before giving up (Y) follow:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 70%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Subject i:</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Age Xi:</p></td>
<td><p>20</p></td>
<td><p>55</p></td>
<td><p>30</p></td>
</tr>
<tr class="row-odd"><td><p>Number of attempts Yi:</p></td>
<td><p>5</p></td>
<td><p>12</p></td>
<td><p>10</p></td>
</tr>
</tbody>
</table>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/figure1_9.png"><img alt="../_images/figure1_9.png" src="../_images/figure1_9.png" style="width: 595.2px; height: 308.8px;" /></a>
</figure>
<p>In this example, the estimators on the right are better than those on the left.</p>
</div>
<p>We can calculate <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> using the following equations:</p>
<div class="math notranslate nohighlight" id="equation-eq-b1">
<span class="eqno">(7)<a class="headerlink" href="#equation-eq-b1" title="Permalink to this equation">Â¶</a></span>\[\begin{split}b_1 = \frac{\sum(X_i - \bar{X}) (Y_i - \bar{Y})}{\sum(X_i - \bar{X})^2} \\\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-eq-b0">
<span class="eqno">(8)<a class="headerlink" href="#equation-eq-b0" title="Permalink to this equation">Â¶</a></span>\[b_0 = \bar{Y} - b_1 \bar{X}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{X}\)</span> and <span class="math notranslate nohighlight">\(\bar{Y}\)</span> are the means of the <span class="math notranslate nohighlight">\(X_i\)</span> and the <span class="math notranslate nohighlight">\(Y_i\)</span> observations, respectively.</p>
<p>There is an important theorem about the least squares estimators:</p>
<div class="topic">
<p class="topic-title">Gauss-Markov theorem</p>
<p>Under the conditions of the regression model, the least squares estimators <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> are <em>unbiased</em> and have minimum variance among all unbiased linear estimators.</p>
</div>
<p>Because <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> are unbiased estimators, we have:</p>
<div class="math notranslate nohighlight">
\[E[b_0] = \beta_0\]</div>
<div class="math notranslate nohighlight">
\[E[b_1] = \beta_1\]</div>
<p>The second statement of the theorem means that among all unbiased linear estimators, <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> have the smallest variability in repeated samples in which the X levels remain unchanged.</p>
</section>
<section id="point-estimation-of-mean-response">
<h4><a class="toc-backref" href="#id9">Point estimation of mean response</a><a class="headerlink" href="#point-estimation-of-mean-response" title="Permalink to this headline">Â¶</a></h4>
<p>Given sample estimators <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> of the parameters in the regression function:</p>
<div class="math notranslate nohighlight">
\[E[Y] = \beta_0 + \beta_1 X\]</div>
<p>we estimate the regression function as follows:</p>
<div class="math notranslate nohighlight">
\[\hat{Y} = b_0 + b_1X\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is the value of the estimated regression function at the level <span class="math notranslate nohighlight">\(X\)</span> of the predictor variable.</p>
<p>Here are some names and definitions:</p>
<ul class="simple">
<li><p>value of the response variable <span class="math notranslate nohighlight">\(Y\)</span> is called <em>response</em></p></li>
<li><p><span class="math notranslate nohighlight">\(E[Y]\)</span>, the mean of the probability distribution of <span class="math notranslate nohighlight">\(Y\)</span> corresponding to the level <span class="math notranslate nohighlight">\(X\)</span> of the predictor variable, is called the <em>mean response</em></p></li>
<li><dl class="simple">
<dt><span class="math notranslate nohighlight">\(\hat{Y}\)</span> is a point estimator of the mean response at level <span class="math notranslate nohighlight">\(X\)</span></dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(\hat{Y}\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(E[Y]\)</span>, with minimum variance</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><span class="math notranslate nohighlight">\(\hat{Y}_i = b_0 + b_1X_i\)</span> for <span class="math notranslate nohighlight">\(i = 1, ..., n\)</span> is the <em>fitted value</em> for the ith case</dt><dd><ul>
<li><p>Note: <span class="math notranslate nohighlight">\(Y_i\)</span> is the observed value for the ith case</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="residuals">
<h4><a class="toc-backref" href="#id10">Residuals</a><a class="headerlink" href="#residuals" title="Permalink to this headline">Â¶</a></h4>
<p>The ith <em>residual</em> is the difference between the observed value <span class="math notranslate nohighlight">\(Y_i\)</span> and the corresponding fitted value <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[e_i = Y_i - \hat{Y}_i = Y_i - b_0 - b_1X_i\]</div>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/figure1_12.png"><img alt="../_images/figure1_12.png" src="../_images/figure1_12.png" style="width: 471.20000000000005px; height: 231.20000000000002px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Note</p>
<p>The model error term <span class="math notranslate nohighlight">\(\epsilon_i = Y_i - E[Y_i]\)</span> and the residual <span class="math notranslate nohighlight">\(e_i = Y_i - \hat{Y}_i\)</span> are <strong>different</strong>. The former is unknown, because the true regression line, <span class="math notranslate nohighlight">\(E[Y_i]\)</span> is unknown. The latter is known, because the fitted value <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> on the estimated regression line is known.</p>
</div>
</section>
</section>
<section id="estimation-of-error-terms-variance-sigma-2">
<h3><a class="toc-backref" href="#id11">Estimation of Error Terms Variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></a><a class="headerlink" href="#estimation-of-error-terms-variance-sigma-2" title="Permalink to this headline">Â¶</a></h3>
<section id="point-estimator-of-sigma-2">
<h4><a class="toc-backref" href="#id12">Point estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span></a><a class="headerlink" href="#point-estimator-of-sigma-2" title="Permalink to this headline">Â¶</a></h4>
<p>First consider sampling from a single population. The variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> of a single population is estimated by the sample variance <span class="math notranslate nohighlight">\(s^2\)</span>. The sample variance is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    s^2 &amp;= \frac{\text{sum of squares}}{\text{degree of freedom}}\\
        &amp;= \frac{\sum^n_{i=1}(Y_i - \bar{Y})^2}{n-1}
\end{split}
\end{equation}\end{split}\]</div>
<p>The sample variance is often called a <em>mean square</em>.</p>
<p>Now consider the case for regression model. We also calculate the sum of squared first, then divide it by the degree of freedom. The sum of squares, denoted by SSE (<em>error sum of squares</em>, or <em>residual sum of squares</em>), is:</p>
<div class="math notranslate nohighlight">
\[SSE = \sum^n_{i-1} (Y_i - \hat{Y}_i)^2 = \sum^n_{i-1}e_i^2\]</div>
<p>Note that here, the mean of <span class="math notranslate nohighlight">\(Y_i\)</span> depends on the level <span class="math notranslate nohighlight">\(X_i\)</span>; hence different to the previous case, we are subtracting the estimated mean <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> for each level, instead of the sample mean <span class="math notranslate nohighlight">\(\bar{Y}\)</span>.</p>
<p>The sum of squares SSE has <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom, because we need to estimate both <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. The mean square, denoted by MSE (<em>error mean square</em> or <em>residual mean square</em>), is:</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
\begin{split}
    s^2 &amp;= MSE = \frac{SSE}{n-1}
        = \frac{\sum^n_{i-1} (Y_i - \hat{Y}_i)^2}{n-2} = \frac{\sum^n_{i-1}e_i^2}{n-2}
\end{split}
\end{equation}\]</div>
<p>MSE is an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span>, so <span class="math notranslate nohighlight">\(E[MSE] = \sigma^2\)</span>.</p>
</section>
</section>
<section id="normally-distributed-error">
<h3><a class="toc-backref" href="#id13">Normally Distributed Error</a><a class="headerlink" href="#normally-distributed-error" title="Permalink to this headline">Â¶</a></h3>
<p>To set up interval estimates and make tests, we need to make an assumption about the form of the distribution of the <span class="math notranslate nohighlight">\(\epsilon_i\)</span>. The standard assumption is that they are normally distributed:</p>
<div class="math notranslate nohighlight">
\[\epsilon_i \sim_{i.i.d} N(0, \sigma^2)\]</div>
<p>Note: the uncorrelatedness assumption of <span class="math notranslate nohighlight">\(\epsilon_i\)</span> becomes independence. As a result, <span class="math notranslate nohighlight">\(Y_i\)</span> are independent normal random variables.</p>
</section>
</section>
<section id="chapter-2-inferences-in-regression-and-correlation-analysis">
<h2><a class="toc-backref" href="#id14">Chapter 2: Inferences in Regression and Correlation Analysis</a><a class="headerlink" href="#chapter-2-inferences-in-regression-and-correlation-analysis" title="Permalink to this headline">Â¶</a></h2>
<p>Throughout this chapter, we assume the normal error regression model described in Chapter 1 is applicable.</p>
<section id="inferences-on-beta-1">
<h3><a class="toc-backref" href="#id15">Inferences on <span class="math notranslate nohighlight">\(\beta_1\)</span></a><a class="headerlink" href="#inferences-on-beta-1" title="Permalink to this headline">Â¶</a></h3>
<p>Frequently, we are interested in drawing inferences about <span class="math notranslate nohighlight">\(\beta_1\)</span>, the slope of the regression line. We also want to do tests on <span class="math notranslate nohighlight">\(\beta_1\)</span>, particularly in this form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    H_0: \beta_1 &amp;= 0\\
    H_a: \beta_1 &amp;\neq 0
\end{split}
\end{equation}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> not only implies that there is no linear association between X and Y, but also that there is not relation of any type between X and Y, because the probability distributions of Y are then identical at all levels of X.</p>
<section id="sampling-distribution-of-beta-1">
<h4><a class="toc-backref" href="#id16">Sampling distribution of <span class="math notranslate nohighlight">\(\beta_1\)</span></a><a class="headerlink" href="#sampling-distribution-of-beta-1" title="Permalink to this headline">Â¶</a></h4>
<p>The point estimator Ref <span class="math notranslate nohighlight">\(b_1\)</span> is given in <a class="reference internal" href="#equation-eq-b1">Eq.7</a> as follows:</p>
<div class="math notranslate nohighlight">
\[b_1 = \frac{\sum(X_i - \bar{X}) (Y_i - \bar{Y})}{\sum(X_i - \bar{X})^2}\]</div>
<p>The sampling distribution of <span class="math notranslate nohighlight">\(b_1\)</span> is the different values of <span class="math notranslate nohighlight">\(b_1\)</span> that would be obtained with repeated sampling when the levels of X are held constant.</p>
<p>For normal error regression model, the sampling distribution of <span class="math notranslate nohighlight">\(b_1\)</span> is normal, with mean and variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    E[b_1] &amp;= \beta_1\\
    \sigma^2[b_1] &amp;= \frac{\sigma^2}{\sum(X_i - \bar{X})^2}
\end{split}
\end{equation}\end{split}\]</div>
<p>The normality of <span class="math notranslate nohighlight">\(b_1\)</span> follows from the fact that <span class="math notranslate nohighlight">\(b_1\)</span> is a linear combination of the <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
<p>We can estimate the variance of <span class="math notranslate nohighlight">\(b_1\)</span> by replacing <span class="math notranslate nohighlight">\(\sigma^2\)</span> with its unbiased estimator <span class="math notranslate nohighlight">\(MSE\)</span>:</p>
<div class="math notranslate nohighlight">
\[s^2[b_1] = \frac{MSE}{\sum(X_i - \bar{X})^2}\]</div>
<p>where the point estimator <span class="math notranslate nohighlight">\(s^2[b_1]\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2[b_1]\)</span>.</p>
</section>
<section id="distribution-of-b-1-beta-1-s-b-1">
<h4><a class="toc-backref" href="#id17">Distribution of <span class="math notranslate nohighlight">\((b_1 - \beta_1) / s[b_1]\)</span></a><a class="headerlink" href="#distribution-of-b-1-beta-1-s-b-1" title="Permalink to this headline">Â¶</a></h4>
<p>Since <span class="math notranslate nohighlight">\(b_1\)</span> is normally distributed, we know that the standardized statistic <span class="math notranslate nohighlight">\((b_1 - \beta_1) / \sigma[b_1]\)</span> has standard normal distribution.</p>
<p>We need to estimate <span class="math notranslate nohighlight">\(\sigma[b_1]\)</span> by <span class="math notranslate nohighlight">\(s[b_1]\)</span>, hence we are interested in the studentized statistic <span class="math notranslate nohighlight">\((b_1 - \beta_1) / s[b_1]\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{b_1 - \beta_1}{s[b_1]} \sim t(n-2),\]</div>
<p>which is a t distribution with <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom.</p>
</section>
<section id="confidence-interval-for-beta-1">
<h4><a class="toc-backref" href="#id18">Confidence interval for <span class="math notranslate nohighlight">\(\beta_1\)</span></a><a class="headerlink" href="#confidence-interval-for-beta-1" title="Permalink to this headline">Â¶</a></h4>
<p>The  <span class="math notranslate nohighlight">\(1-\alpha\)</span> confidence interval for <span class="math notranslate nohighlight">\(\beta_1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[[b_1 - t(1-\alpha/2; n-2) s[b_1]], b_1 + t(1-\alpha/2; n-2) s[b_1]],\]</div>
<p>where <span class="math notranslate nohighlight">\(t(1-\alpha/2; n-2)\)</span> denotes the <span class="math notranslate nohighlight">\((\alpha/2)100\)</span> percentile of the t distribution with n-2 degrees of freedom. For example, for a 95 percent confidence interval with sample size of 25, we have <span class="math notranslate nohighlight">\(t(.975; 23) = 2.069\)</span>.</p>
<p>This can also be stated in a probability statement:</p>
<div class="math notranslate nohighlight">
\[P[t(\alpha/2; n-2) \leq \frac{b_1 - \beta_1}{s[b_1]} \leq t(1-\alpha/2; n-2)] = 1 - \alpha\]</div>
</section>
<section id="hypothesis-test-for-beta-1">
<h4><a class="toc-backref" href="#id19">Hypothesis test for <span class="math notranslate nohighlight">\(\beta_1\)</span></a><a class="headerlink" href="#hypothesis-test-for-beta-1" title="Permalink to this headline">Â¶</a></h4>
<p>Consider a two-sided test with the test alternatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    H_0: \beta_1 &amp;= 0\\
    H_a: \beta_1 &amp;\neq 0
\end{split}
\end{equation}\end{split}\]</div>
<p>We have the test statistic:</p>
<div class="math notranslate nohighlight">
\[t^* = \frac{b_1}{s[b_1]} \sim t(n-2)\]</div>
<p>and the decision rules is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    &amp;\text{If } |t^*| \leq t(1-\alpha/2; n-2)\text{, reject }H_0\\
    &amp;\text{If } |t^*| &gt; t(1-\alpha/2; n-2)\text{, fail to reject }H_0
\end{split}
\end{equation}\end{split}\]</div>
<p>The P-value is the probability <span class="math notranslate nohighlight">\(P[t(n-2) &gt; t^*]\)</span>. If P-value is less than the level of significance <span class="math notranslate nohighlight">\(\alpha\)</span>, we reject the null hypothesis.</p>
<p>P-value is interpreted as follows: if <span class="math notranslate nohighlight">\(b_1 = 5\)</span>, the P-value is the probability of observing <span class="math notranslate nohighlight">\(b_1\)</span> at least far away from 0 (<span class="math notranslate nohighlight">\(b_1 \geq 5\)</span> or <span class="math notranslate nohighlight">\(b_1 \leq -5\)</span>) assuming that the Null hypothesis is true.</p>
</section>
</section>
<section id="inferences-on-beta-0">
<h3><a class="toc-backref" href="#id20">Inferences on <span class="math notranslate nohighlight">\(\beta_0\)</span></a><a class="headerlink" href="#inferences-on-beta-0" title="Permalink to this headline">Â¶</a></h3>
<p>We only consider the case when the scope of the model includes <span class="math notranslate nohighlight">\(X=0\)</span>.</p>
<section id="id1">
<h4><a class="toc-backref" href="#id21">Sampling distribution of <span class="math notranslate nohighlight">\(\beta_1\)</span></a><a class="headerlink" href="#id1" title="Permalink to this headline">Â¶</a></h4>
<p>The point estimator Ref <span class="math notranslate nohighlight">\(b_0\)</span> is given in <a class="reference internal" href="#equation-eq-b0">Eq.8</a> as follows:</p>
<div class="math notranslate nohighlight">
\[b_0 = \bar{Y} - b_1 \bar{X}\]</div>
<p>For normal error regression model, the sampling distribution of <span class="math notranslate nohighlight">\(b_1\)</span> is normal, with mean and variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    E[b_0] &amp;= \beta_0\\
    \sigma^2[b_0] &amp;= \sigma^2 \left[ \frac{1}{n} + \frac{\bar{X}^2}{\sum(X_i - \bar{X})^2} \right]
\end{split}
\end{equation}\end{split}\]</div>
<p>The normality of <span class="math notranslate nohighlight">\(b_0\)</span> follows from the fact that <span class="math notranslate nohighlight">\(b_0\)</span> is a linear combination of the <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
<p>We can estimate the variance of <span class="math notranslate nohighlight">\(b_0\)</span> by replacing <span class="math notranslate nohighlight">\(\sigma^2\)</span> with its unbiased estimator <span class="math notranslate nohighlight">\(MSE\)</span>:</p>
<div class="math notranslate nohighlight">
\[s^2[b_0] = MSE \left[ \frac{1}{n} + \frac{\bar{X}^2}{\sum(X_i - \bar{X})^2} \right]\]</div>
<p>where the point estimator <span class="math notranslate nohighlight">\(s^2[b_0]\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2[b_0]\)</span>.</p>
</section>
<section id="confidence-interval-for-beta-0">
<h4><a class="toc-backref" href="#id22">Confidence interval for <span class="math notranslate nohighlight">\(\beta_0\)</span></a><a class="headerlink" href="#confidence-interval-for-beta-0" title="Permalink to this headline">Â¶</a></h4>
<p>The  <span class="math notranslate nohighlight">\(1-\alpha\)</span> confidence interval for <span class="math notranslate nohighlight">\(\beta_0\)</span> is:</p>
<div class="math notranslate nohighlight">
\[[b_0 - t(1-\alpha/2; n-2) s[b_0]], b_0 + t(1-\alpha/2; n-2) s[b_0]]\]</div>
</section>
</section>
<section id="power-of-tests">
<h3><a class="toc-backref" href="#id23">Power of Tests</a><a class="headerlink" href="#power-of-tests" title="Permalink to this headline">Â¶</a></h3>
<p>Consider the general test concerning <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    H_0: \beta_1 &amp;= \beta_{10}\\
    H_a: \beta_1 &amp;\neq \beta_{10}
\end{split}
\end{equation}\end{split}\]</div>
<p>The <strong>power</strong> of this test is the probability that the decision rule will lead to conclusion <span class="math notranslate nohighlight">\(H_a\)</span> when <span class="math notranslate nohighlight">\(H_a\)</span> holds:</p>
<div class="math notranslate nohighlight">
\[Power = P(|t^*| &gt; t(1-\alpha/2; n-2) | \delta),\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta\)</span> is the <em>noncentrality measure</em> - how far the true value of <span class="math notranslate nohighlight">\(\beta_1\)</span> is from <span class="math notranslate nohighlight">\(\beta_{10}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\delta = \frac{\beta_1 - \beta_{10}}{\sigma{\beta_1}}\]</div>
</section>
<section id="interval-estimation-of-e-y-h">
<h3><a class="toc-backref" href="#id24">Interval Estimation of <span class="math notranslate nohighlight">\(E[Y_h]\)</span></a><a class="headerlink" href="#interval-estimation-of-e-y-h" title="Permalink to this headline">Â¶</a></h3>
<p>A common objective in regression analysis is to estimate the mean for probability distributions of Y for various levels of X. Let <span class="math notranslate nohighlight">\(X_h\)</span> denote the level of X. The mean response when <span class="math notranslate nohighlight">\(X = X_h\)</span> is denoted by <span class="math notranslate nohighlight">\(E[Y_h]\)</span>.  <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> is the point estimator of <span class="math notranslate nohighlight">\(E[Y_h]\)</span> and is given by:</p>
<div class="math notranslate nohighlight">
\[\hat{Y}_h = b_0 + b_1X_h\]</div>
<section id="sampling-distribution-of-hat-y-h">
<h4><a class="toc-backref" href="#id25">Sampling Distribution of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span></a><a class="headerlink" href="#sampling-distribution-of-hat-y-h" title="Permalink to this headline">Â¶</a></h4>
<p>The sampling distribution of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> refers to the different values of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> that would be obtained if repeated samples were selected, each holding the levels of the predictor variable X constant, and calculating <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> for each sample.</p>
<p>For normal error regression model, the sampling distribution of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> is normal, with mean and variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    E[\hat{Y}_h] &amp;= E[Y_h]\\
    \sigma^2[\hat{Y}_h] &amp;= \sigma^2 \left[ \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum(X_i - \bar{X})^2} \right]
\end{split}
\end{equation}\end{split}\]</div>
<p><strong>Normality.</strong> The normality of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> follows from the fact that <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> is a linear combination of the <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
<p><strong>Mean.</strong> <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(E[Y_h]\)</span>.</p>
<p><strong>Variance.</strong> The variance of the sampling distribution of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> is affected by how far <span class="math notranslate nohighlight">\(X_h\)</span> is from <span class="math notranslate nohighlight">\(\bar{X}\)</span>. The further from <span class="math notranslate nohighlight">\(\bar{X}\)</span> is <span class="math notranslate nohighlight">\(X_h\)</span>, the larger is the variance of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span>.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/figure2_3.png"><img alt="../_images/figure2_3.png" src="../_images/figure2_3.png" style="width: 428.8px; height: 206.4px;" /></a>
</figure>
<p>We can estimate the variance of <span class="math notranslate nohighlight">\(\hat{Y}_h\)</span> by replacing <span class="math notranslate nohighlight">\(\sigma^2\)</span> with its unbiased estimator <span class="math notranslate nohighlight">\(MSE\)</span>:</p>
<div class="math notranslate nohighlight">
\[s^2[\hat{Y}_h] = MSE \left[ \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum(X_i - \bar{X})^2} \right]\]</div>
<p>where the point estimator <span class="math notranslate nohighlight">\(s^2[\hat{Y}_h]\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2[\hat{Y}_h]\)</span>.</p>
</section>
<section id="confidence-interval-for-e-y-h">
<h4><a class="toc-backref" href="#id26">Confidence interval for <span class="math notranslate nohighlight">\(E[Y_h]\)</span></a><a class="headerlink" href="#confidence-interval-for-e-y-h" title="Permalink to this headline">Â¶</a></h4>
<p>The  <span class="math notranslate nohighlight">\(1-\alpha\)</span> confidence interval for <span class="math notranslate nohighlight">\(E[Y_h]\)</span> is:</p>
<div class="math notranslate nohighlight">
\[[\hat{Y}_h - t(1-\alpha/2; n-2) s[\hat{Y}_h]], \hat{Y}_h + t(1-\alpha/2; n-2) s[\hat{Y}_h]]\]</div>
</section>
</section>
<section id="prediction-of-new-observation">
<h3><a class="toc-backref" href="#id27">Prediction of New Observation</a><a class="headerlink" href="#prediction-of-new-observation" title="Permalink to this headline">Â¶</a></h3>
<p>The new observation of y to be predicted is viewed as the result of a new trial, independent of the trials on which the regression analysis is based. We denote the level of X for the new trial as <span class="math notranslate nohighlight">\(X_h\)</span> and the new observation on Y as <span class="math notranslate nohighlight">\(Y_{h(new)}\)</span></p>
<p>The difference between estimation of the mean response <span class="math notranslate nohighlight">\(E[Y_h]\)</span> and prediction of a new response <span class="math notranslate nohighlight">\(Y_{h(new)}\)</span> is that, the former estimates the <em>mean</em> of the distribution of Y, and the latter predicts an <em>individual outcome</em> drawn from the distribution of Y.</p>
<div class="topic">
<p class="topic-title">Recap: If the true regression line is known, how to predict new observation?</p>
<p>Lets assume  the regression parameters are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    \beta_0 &amp;= 0.10 \quad \beta_1 = 0.95 \\
    E[Y] &amp;= 0.10 + 0.95 X\\
    \sigma &amp;= 0.12
\end{split}
\end{equation}\end{split}\]</div>
<p>Given <span class="math notranslate nohighlight">\(X_h = 3.5\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[E[Y_h] = 0.10 + 0.95 \times 3.5 = 3.425\]</div>
<p>The 99.7% confidence interval includes area that is within 3 standard deviations from the mean. Hence we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    3.425 - 3 \times 0.12 &amp;\leq Y_{h(new)} \leq 3.425 + 3 \times 0.12 \\
    3.065 &amp;\leq Y_{h(new)} \leq 3.785
\end{split}
\end{equation}\end{split}\]</div>
<p>In general, the <span class="math notranslate nohighlight">\(1-\alpha\)</span> prediction limits for <span class="math notranslate nohighlight">\(Y_{h(new)}\)</span> are:</p>
<div class="math notranslate nohighlight">
\[E[Y_h] \pm z(1-\alpha/2)\sigma\]</div>
</div>
<p>When the true regression line is unknown, we need to take into account two sources of variations:</p>
<ol class="arabic simple">
<li><p>Variation of estimated mean of Y given X (where is the distribution of Y located)</p></li>
<li><p>Variation of Y around a particular mean (variation within the distribution of Y)</p></li>
</ol>
<p>Variation 1 comes from the fact that we are estimating the regression line. Recall that the confidence interval for <span class="math notranslate nohighlight">\(E[Y_h]\)</span> is</p>
<div class="math notranslate nohighlight">
\[[\hat{Y}_h - t(1-\alpha/2; n-2) s[\hat{Y}_h], \hat{Y}_h + t(1-\alpha/2; n-2) s[\hat{Y}_h]]\]</div>
<p>Variance 2 comes from the error terms in the regression model.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/figure2_5.png"><img alt="../_images/figure2_5.png" src="../_images/figure2_5.png" style="width: 548.8000000000001px; height: 251.20000000000002px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Theorem: the studentized statistic</p>
<p><span class="math notranslate nohighlight">\(\frac{Y_{h(new)} - \hat{Y}_h}{s[spred]}\)</span> is distributed as t(n-2) for normal error regression model, and</p>
<ul>
<li><p>the <span class="math notranslate nohighlight">\(1-\alpha\)</span> prediction limits are</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\hat{Y}_h \pm t(1-\alpha/2; n-2) s[pred]\]</div>
</div></blockquote>
</li>
<li><p>the variance of the prediction error is <span class="math notranslate nohighlight">\(\sigma^2[pred] = \sigma^2 + \sigma^2[\hat{Y}_h]\)</span></p></li>
<li><p>an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2 [pred]\)</span> is:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[s^2[pred] = MSE + s^2[\hat{Y}_h]\]</div>
</div></blockquote>
</li>
</ul>
</div>
</section>
<section id="confidence-band-for-regression-line">
<h3><a class="toc-backref" href="#id28">Confidence Band for Regression Line</a><a class="headerlink" href="#confidence-band-for-regression-line" title="Permalink to this headline">Â¶</a></h3>
</section>
<section id="anova-analysis-of-variance-approach-to-regression-analysis">
<h3><a class="toc-backref" href="#id29">ANOVA (Analysis of Variance) Approach to Regression Analysis</a><a class="headerlink" href="#anova-analysis-of-variance-approach-to-regression-analysis" title="Permalink to this headline">Â¶</a></h3>
<p>The analysis of variance approach is based on the partitioning of sums of squares and degrees of freedom associated with the response variable Y.</p>
<section id="partitioning-of-total-sum-of-squares">
<h4><a class="toc-backref" href="#id30">Partitioning of Total Sum of Squares</a><a class="headerlink" href="#partitioning-of-total-sum-of-squares" title="Permalink to this headline">Â¶</a></h4>
<p>First we define three notions:</p>
<ol class="arabic">
<li><p><strong>SSTO</strong>: total sum of squares. This measures the deviation of data <span class="math notranslate nohighlight">\(Y_i\)</span> around their mean <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, disregarding how large <span class="math notranslate nohighlight">\(X\)</span> is.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[SSTO = \sum(Y_i - \bar{Y})^2\]</div>
</div></blockquote>
</li>
<li><p><strong>SSE</strong>: error sum of squares. This reflects the uncertainty of Y around the fitted regression line. In this case, X is taken into account. <span class="math notranslate nohighlight">\(SSE = 0\)</span> if all the observations fall on the fitted regression line.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[SSE = \sum(Y_i - \hat{Y}_i)^2\]</div>
</div></blockquote>
</li>
<li><p><strong>SSR</strong>: regression sum of squares. This is the difference between the fitted value on the regression line and the mean of the fitted values. If the regression line is horizontal and equals to the mean, <span class="math notranslate nohighlight">\(SSR = 0\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[SSR = \sum(\hat{Y}_i - \bar{Y})^2\]</div>
</div></blockquote>
</li>
</ol>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/figure2_7.png"><img alt="../_images/figure2_7.png" src="../_images/figure2_7.png" style="width: 632.8000000000001px; height: 307.20000000000005px;" /></a>
</figure>
<p>More formally, the total deviation can be partitioned into two parts:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/figure_deviation.png"><img alt="../_images/figure_deviation.png" src="../_images/figure_deviation.png" style="width: 211.20000000000002px; height: 110.4px;" /></a>
</figure>
<p>As the result, the total sum of squares can also be partitioned into two parts:</p>
<div class="math notranslate nohighlight">
\[SSTO = SSR + SSE\]</div>
</section>
<section id="breakdown-of-degrees-of-freedom">
<h4><a class="toc-backref" href="#id31">Breakdown of Degrees of Freedom</a><a class="headerlink" href="#breakdown-of-degrees-of-freedom" title="Permalink to this headline">Â¶</a></h4>
<p>Corresponding to the partitioning of the total sum of squares SSTO, there is a partitioning of the associated degrees of freedom (df):</p>
<ul class="simple">
<li><p>SSTO has n - 1 degrees of freedom. One is lost because we use the sample mean <span class="math notranslate nohighlight">\(\bar{Y}\)</span> to estimate the population mean.</p></li>
<li><p>SSE has n - 2 degrees of freedom. Two are lost because <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are estimated.</p></li>
<li><p>SSR has one degree of freedom. The deviations <span class="math notranslate nohighlight">\(\hat{Y}_i - \bar{Y}\)</span> are calculated from the regression line, which has 2 degrees of freedom, coreesponding to the intercept and the slope. One is lost because <span class="math notranslate nohighlight">\(\bar{Y}\)</span> is estimated.</p></li>
</ul>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
    df(SSTO) &amp;= df(SSE) + df(SSR)\\
    n-1 &amp;= (n+2) + 1
\end{split}
\end{equation}\end{split}\]</div>
</section>
<section id="mean-squares">
<h4><a class="toc-backref" href="#id32">Mean Squares</a><a class="headerlink" href="#mean-squares" title="Permalink to this headline">Â¶</a></h4>
<p>A sum of squares divided by its degrees of freedom is called a <em>mean square</em>. We have the following notions:</p>
<ol class="arabic simple">
<li><p><strong>MSE</strong>: error mean square.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[MSE = \frac{SSE}{n-2}\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>MSR</strong>: regression mean square.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[MSR = \frac{SSR}{1} = SSR\]</div>
</section>
<section id="anova-table">
<h4><a class="toc-backref" href="#id33">ANOVA table</a><a class="headerlink" href="#anova-table" title="Permalink to this headline">Â¶</a></h4>
<p>The above notions are usually summarized in a table:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/table_2_2.png"><img alt="../_images/table_2_2.png" src="../_images/table_2_2.png" style="width: 627.2px; height: 161.60000000000002px;" /></a>
</figure>
</section>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../Java/jsp.html" title="JavaServer Pages (JSP) Tutorial"
             >next</a> |</li>
        <li class="right" >
          <a href="neural_networks.html" title="Neural Networks"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Notebook  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Regression Analysis</a></li> 
      </ul>
    </div>
    
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019, Yixuan Ni.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.1.2.
    </div>
    

  </body>
</html>