


<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Hidden Markov Models &#8212; Notebook  documentation</title>
    <link rel="stylesheet" href="_static/yeen.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Reading Log and Quotes" href="reading.html" />
    <link rel="prev" title="First-Order Theroies" href="first_order_theory.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="reading.html" title="Reading Log and Quotes"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="first_order_theory.html" title="First-Order Theroies"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Notebook  documentation</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Hidden Markov Models</a><ul>
<li><a class="reference internal" href="#markov-models">Markov Models</a><ul>
<li><a class="reference internal" href="#first-order-markov-chain">First-order Markov chain</a></li>
<li><a class="reference internal" href="#higher-order-markov-chain">Higher-order Markov chain</a></li>
<li><a class="reference internal" href="#latent-variables">Latent variables</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id1">Hidden Markov Models</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="first_order_theory.html"
                        title="previous chapter">First-Order Theroies</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="reading.html"
                        title="next chapter">Reading Log and Quotes</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/hmm.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="hidden-markov-models">
<h1><a class="toc-backref" href="#id2">Hidden Markov Models</a><a class="headerlink" href="#hidden-markov-models" title="Permalink to this headline">¶</a></h1>
<p>A summary of Chapter 13 Sequential Data of <em>Pattern Recognition and Machine Learning - Springer 2006</em>.</p>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#hidden-markov-models" id="id2">Hidden Markov Models</a></p>
<ul>
<li><p><a class="reference internal" href="#markov-models" id="id3">Markov Models</a></p>
<ul>
<li><p><a class="reference internal" href="#first-order-markov-chain" id="id4">First-order Markov chain</a></p></li>
<li><p><a class="reference internal" href="#higher-order-markov-chain" id="id5">Higher-order Markov chain</a></p></li>
<li><p><a class="reference internal" href="#latent-variables" id="id6">Latent variables</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#id1" id="id7">Hidden Markov Models</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="markov-models">
<h2><a class="toc-backref" href="#id3">Markov Models</a><a class="headerlink" href="#markov-models" title="Permalink to this headline">¶</a></h2>
<p>Using the product rule, we can express the joint distribution for a sequence of obervations in the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x_1, ..., x_N) &amp;= \prod\limits_{n=1}^N p(x_1, ... x_{n-1})\\
                 &amp;= p(x_1) p(x_2|x_1) p(x_3|x_2, x_1) ...\end{split}\]</div>
<div class="section" id="first-order-markov-chain">
<h3><a class="toc-backref" href="#id4">First-order Markov chain</a><a class="headerlink" href="#first-order-markov-chain" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><dl class="simple">
<dt><strong>Definition:</strong> first-order Markov chain</dt><dd><p>If we assume that each of the conditional distributions on the RHS is independent of all previousobservations except the most recent (the Markov Property), we obtain the <em>first-order Markov chain</em>:</p>
</dd>
</dl>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}p(x_1, ..., x_N) &amp;= p(x_1)\prod\limits_{n=2}^N p(x_n|x_{n-1})\\
                 &amp;= p(x_1) p(x_2|x_1) p(x_3|x_2) ...\end{split}\]</div>
<div class="topic">
<p class="topic-title first">Illustration of first-order Markov chain</p>
<div class="figure align-center">
<img alt="_images/first_order_markov_chain.png" src="_images/first_order_markov_chain.png" />
</div>
</div>
<p>The conditional distribution for observation <span class="math notranslate nohighlight">\(x_n\)</span>, given all of the observations up to time <span class="math notranslate nohighlight">\(n\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[p(x_n | x_1, ..., x_{n-1}) = p(x_n|x_{n-1})\]</div>
<p>If we use this model to predict the next observation in a sequence, the distribution of predictions will only depend on the value of the immediately preceding observation, and will be independent of all earlier observations.</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>Definiton:</strong> homogeneous Markov chain</dt><dd><p>A markov chain is <em>homogeneous</em> when we assume a stationary time series, i.e. conditional distributions <span class="math notranslate nohighlight">\(p(x_n|x_{n-1})\)</span> are equal.</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="higher-order-markov-chain">
<h3><a class="toc-backref" href="#id5">Higher-order Markov chain</a><a class="headerlink" href="#higher-order-markov-chain" title="Permalink to this headline">¶</a></h3>
<p>Sometimes the trends in the data over several successive observations will provide important information in predicting the next value. We can increase the number of obeservations the predictions depend on and obtain higher-order Markov chains.</p>
<p>For example, the joint distribution of the second-order Markov chain is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x_1, ..., x_N) &amp;= p(x_1)p(x_2|x_1)\prod\limits_{n=3}^N p(x_n|x_{n-1}, x_{n-2})\\
             &amp;= p(x_1) p(x_2|x_1) p(x_3|x_2, x_1) p(x_4|x_3, x_2) ...\end{split}\]</div>
<div class="topic">
<p class="topic-title first">Illustration of second-order Markov chain</p>
<div class="figure align-center">
<img alt="_images/second_order_markov_chain.png" src="_images/second_order_markov_chain.png" />
</div>
</div>
<p>More generally, we have <span class="math notranslate nohighlight">\(M^{th}\)</span>-order markov chain where</p>
<div class="math notranslate nohighlight">
\[p(x_n |x_1, ... x_{n-1}) = p(x_n| x_{n-1}, ... x_{n-M})\]</div>
<ul class="simple">
<li><p><em>Note</em> the incresed flexibility results in a model with a much larger number of parameters (can grow exponentially).</p></li>
</ul>
</div>
<div class="section" id="latent-variables">
<h3><a class="toc-backref" href="#id6">Latent variables</a><a class="headerlink" href="#latent-variables" title="Permalink to this headline">¶</a></h3>
<p>Suppose we do not want to limit our model to the Markov assumption and yet can be specified using a limited number of free parameters, we can introduce <em>latent variables</em> to permit a rich class of models to be constructed out of simple components. (Similar approach: mixture distributions, continuous latent variable models, etc.)</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>Definition</strong> state space model</dt><dd><p>For each observation <span class="math notranslate nohighlight">\(x_n\)</span>, we introduce a corresponding latent variable <span class="math notranslate nohighlight">\(z_n\)</span> (can have different dimentionality/type). If we assume that the latent variables form a Markov chain, we have a graphical structure called <em>state space model</em>.</p>
</dd>
</dl>
</li>
</ul>
<div class="topic">
<p class="topic-title first">State space model</p>
<div class="figure align-center">
<img alt="_images/markov_chain_latent.png" src="_images/markov_chain_latent.png" />
</div>
</div>
<p>It satisfies the key conditional independence property that <span class="math notranslate nohighlight">\(z_{n-1}\)</span> and <span class="math notranslate nohighlight">\(z_{n+1}\)</span> are <strong>independent</strong> given <span class="math notranslate nohighlight">\(z_n\)</span>, and the joint distribution of the model is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x_1, ..., x_N, z_1, ... z_N) &amp;= p(z_1) (\prod\limits_{n=2}^N p(z_n|z_{n-1})) (\prod\limits_{n=1}^N p(x_n|z_n))\\
                               &amp;= p(z_1)p(z_2|z_1)p(z_3|z_2)...p(x_1|z_1)p(x_2|z_2)p(x_3|z_3)...\end{split}\]</div>
</div>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id7">Hidden Markov Models</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="reading.html" title="Reading Log and Quotes"
             >next</a> |</li>
        <li class="right" >
          <a href="first_order_theory.html" title="First-Order Theroies"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Notebook  documentation</a> &#187;</li> 
      </ul>
    </div>
    
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019, Sherry.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.0.1.
    </div>
    

  </body>
</html>